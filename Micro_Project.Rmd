---
title: "Microeconometrics, Empirical project, Group 8"
author:
- Atanasov Georgi^[student ID 11776393]
- Fitter Jonathan^[student ID 11709902]
- Hochholzer Matthias^[student ID 11724853]
- Woharcik Verena^[student ID 11701581]
date: "17th February 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lastpage}
- \usepackage{graphicx}
- \pagestyle{fancy}
- \fancypagestyle{plain}{\pagestyle{fancy}}
- \fancyhead[L]{Empirical project}
- \fancyhead[R]{\thepage\ of \pageref{LastPage}}
- \fancyfoot[R]{\includegraphics[width=3cm]{Uni_Logo_blau.png}}
- \fancyfoot[C]{}
- \setlength{\footskip}{46.27646pt}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
#Loading packages
library(tidyverse)
library(foreign)
library(ggplot2)
library(ROCR)
library(ivpack)
```


## Importing data
from Wooldridge, his source: J. Grogger (1991), “Certainty vs. Severity of Punishment,” Economic Inquiry
29, 297-309.
```{r}
df<-read.dta("http://fmwww.bc.edu/ec-p/data/wooldridge/crime1.dta")
attach(df)
head(df)
str(df)
summary(df)
```
A data.frame with 2725 observations on 16 variables:
- narr86: times arrested, 1986
- nfarr86: felony arrests, 1986
- nparr86: property crme arr., 1986
- pcnv: proportion of prior convictions
- avgsen: avg sentence length, mos.
- tottime: time in prison since 18 (mos.)
- ptime86: mos. in prison during 1986
- qemp86: quarters employed, 1986
- inc86: legal income, 1986, $100s
- durat: recent unemp duration
- black: =1 if black
- hispan: =1 if Hispanic
- born60: =1 if born in 1960
- pcnvsq: pcnv^2
- pt86sq: ptime86^2
- inc86sq: inc86^2

# Descriptive Statistics

### Correlation Plots

```{r}
plot(df[,c("narr86", "avgsen", "inc86", "durat")])
cor(df[,c("narr86", "avgsen", "inc86", "durat")])
```

### Specific Plots:

```{r, echo=FALSE}
plot(narr86, inc86, main = "Correlation, crime 1986", xlab= "times arrested", ylab="legal income", col="darkblue")
```

```{r, echo=FALSE}
plot(narr86, avgsen, main = "Correlation, crime 1986", xlab= "times arrested", ylab="avg sentence length, mos.", col="darkblue")
```

```{r}
"HISTOGRAMME !"
```


"NEEDS CHANGE!!!!!!!!!"
!
!
!
```{r, echo=FALSE}
barplot(table(ifelse(df$black==1,df$narr86,NA)), xlab = "times arrested", ylab = "amount", main = "Histogram Black/White", col="blue")
barplot(table(ifelse(df$black==0,df$narr86,NA)), xlab = "times arrested", ylab = "amount", main = "Histogram Black/White", col="red")
```

#PART 1

##  ** Modeling "avgsen" **
Building model estimating expected severity of conviction when arrested in 1986 using level of income, employment, total time spend in prison and color (black ad non-black) of the arrested

Our hypothesis is, that the mentioned variables have a significant effect on the average sentence length.

$$  avgsen = \beta_0 + \beta_1\ inc86 + \beta_2\ black +  \beta_3\ tottime +  \beta_4\ qemp86  $$ 

### Simple OLS-Estimation

 A General OLS estimation including all potential regressors:
```{r,include=TRUE}
lm_all<-lm(avgsen~. -nfarr86 - nparr86 , data = df)
summary(lm_all)
```
Interpretation:
A high R-squared is observable. Only few variables are significant for 0.05 and 0.1 significance level. Also the p-Value of the F-statistic is low, which implies that there are some variables which can be used to explain the average sentence length.

We have proceeded our further estimation of avsen after excluding variables which have considerably high p-values.

The average severity is regressed on the income in 1986, employment in 1986, color (black and non-black) and total time spend in prison.
```{r,include=TRUE}
lm_sev<-lm(avgsen~ tottime+ black+ qemp86+ inc86, data = df) 
```
An output of an OLS-Estimation is given:
```{r}
summary((lm_sev))
```
Interpretation:
We see almost the same R-squared as from the previous OLS-Estimation. The significant variables for 0.05 significance level are the total time spend in prison and the color. No significance of the other variables is proven.

#### Problems with the OLS 
Some of the variables may be endogenous E.g assumptions may be violated.
=> Testing  this way may not be correct.

#################################
#########LATEX EQUATION##########
#################################

### IV-Regression (using 2SLS-Estimation)
Use instrumental variables in the estimation of the expected severity.
Define:
endogenous var: income86, qemp86, tottime
exogenuos var: black
instruments: durat, nparr, nfarr, narr, ptime86

The regression code is given by:
```{r, include=TRUE}
IV_sev1<-ivreg(avgsen~ tottime+ black+ qemp86+ inc86 | black+ durat+ narr86+ nfarr86+ nparr86+ ptime86  ,data=df)

summary(IV_sev1, diagnostics=TRUE)
```
Interpretation:
Here a high R-squared is observed. Tottime and black are the only significant variables for 0.05 significance level. 
Furthermore, diagnostics of the instruments are provided. We observe small p-values, which means that instruments are not weak e.g they are appropriate. The value of the Hausmans-test is smaller than than the significance level of 0.05. Thus, meaning that instruments and residuals can be considered as uncorrelated. 


#### Manual Check if Instuments are adequate
1. Check if regressors and instruments are correlated
```{r ,include=TRUE}
i1lm_sev1<- lm(tottime~ black+ durat+ narr86+ nfarr86+ nparr86+ ptime86, data=df)
summary(i1lm_sev1)

i2lm_sev1<- lm(qemp86~ black+ durat+ narr86+ nfarr86+ nparr86+ ptime86, data=df)
summary(i2lm_sev1)

i3lm_sev1<- lm( inc86~ black+ durat+ narr86+ nfarr86+ nparr86+ ptime86, data=df)
summary(i3lm_sev1)
```
R-squared >> 0 is observed in every regression => first criterion is met.  

2. Check if errors and instruments are uncorrelated.
```{r, include=TRUE}
resid_sev1<-resid(IV_sev1)
lm_resid_sev1<-lm(resid_sev1~black+ durat+ narr86+ nfarr86+ nparr86+ ptime86, data=df)
summary(lm_resid_sev1)
```
A really small R-squared is observed. The p-values of variables are considerably higher than 0.05 significance level. 

What can be done in addition is a test on $ n*R^{2} $ , where  $ R^{2} $ is the non-centered $ R^{2} $ ($ R^{2} $ used)
```{r, include=TRUE}
summary(lm_resid_sev1)$r.squared*length(resid_sev1)
```
Value is smaller than the Chi-square value on 2 df and 0.05 significance level=> also the second criterion is met. 



# PART 2
Building a model, which aims at estimating probability of arrest during 1986. A dependend binory variable, describing the states: arrested and not arrested, is to be regressed. 

In this part we test the hypothesis that every single regressor has a significant impact on the dependend variable.

## Simple OLS Regression, LPM

### OLS estimation of the variable narr86

Regressing the variable narr86 on almost all variables
```{r, echo=FALSE}
# with all viariables
ols_all <- lm(narr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86 + inc86 + durat + black + hispan + born60 + pcnvsq + pt86sq + inc86sq , data=df)

summary(ols_all)
```
We will proceed our estimations ommitting insignificant variables from this estimation.

###  The Chosen Model:

After omitting the insignificant variables, we create the following model:

$$ narr86 = \beta_0 + \beta_1\ pcnv + \beta_2\ ptime86 +  \beta_3\ inc86 +  \beta_4\ black +  \beta_5\ hispan +  \beta_6\ pcnvsq+ \beta_7\ pt86sq+ \beta_8\ inc86sq   $$

```{r, echo=FALSE}
#with chosen variables
ols <- lm(narr86 ~ pcnv + ptime86 + inc86 + black + hispan + pcnvsq + pt86sq + inc86sq  , data=df)
summary(ols)
#becuase of problems
robust.se(ols)
```

Interpretation:
First to notice is the neglection of parameter restrictions: E.g. negative values cannot easily be interpreted in this scenario.

Although OLS yields unbiased estimators, heteroskedasticity among other things leads to inefficient ones. 

Additionally: Errors also not normal

## LOGIT model
We are creating a binary variable arr86, when a person gets arrested at least once.
Define: 
arr86 = 1 if arrested in 1986
arr86 = 0 if not arrested in 1986
```{r}
df$arr86 <- ifelse(df$narr86>0 ,1 ,0)
```

We create a Logit-Model with all variables
```{r}
log_all <- glm(arr86 ~ pcnv + avgsen + tottime + ptime86 + qemp86 + inc86 + durat + black + hispan + born60 + pcnvsq + pt86sq + inc86sq , data = df, family=binomial(link = "logit"))

summary(log_all)
```

Further Logit-Model with reduced number of variables:
#############################qepmpl not included)(pcnv included) in the Latex###############
##########################################################################################
#########################################################################################


$$ Pr(arr86=1|X) = \frac{exp(\beta_0 + \beta_1\ pcnv + \beta_2\ ptime86 +  \beta_3\ inc86 +  \beta_4\ black +  \beta_5\ hispan +  \beta_6\ pcnvsq+ \beta_7\ pt86sq+ \beta_8\ inc86sq)}{1+exp(\beta_0 + \beta_1\ pcnv + \beta_2\ ptime86 +  \beta_3\ inc86 +  \beta_4\ black +  \beta_5\ hispan +  \beta_6\ pcnvsq+ \beta_7\ pt86sq+ \beta_8\ inc86sq)}   $$

```{r}
log <- glm(arr86 ~   ptime86 + qemp86 + inc86  + black + hispan + pcnvsq + pt86sq + inc86sq , data = df, family=binomial(link = "logit"))

summary(log)

```


For comparison a Probit-Model with same regressors is given:
```{r}
prob <- glm(arr86 ~   ptime86 + qemp86 + inc86  + black + hispan + pcnvsq + pt86sq + inc86sq , data = df, family=binomial(link = "probit"))

summary(prob)
```
### Models diagnostics
## Calculation of MC Faddens pseudo R^2
```{r}
r_log<- 1-(log$deviance/log$null.deviance)

r_prob<- 1-(prob$deviance/prob$null.deviance)
```
MC Faddens pseudo R^2 for Logit is `r_log` and for Probit it is `r_prob`.


## Scaling of probit to logit (ptime86)
```{r, include=FALSE}
factor_log_prob<- (log$coefficients[2]/prob$coefficients[2])
factor_log_prob
```

The factor between our Probit and Logit is `factor_log_prob`. And it is close to 1.6

##Interpretation of Coefficients: Odds and Average-Marginal-Effects
```{r}
# for logit
odds<- exp(log$coefficients)
odds

fav <- mean(dnorm(predict(log,type="link")))
fav*coef(log)
```

## Classification table
```{r}
tab <- table(true= df$arr86, pred= ifelse(fitted(log)>0.5,1,0))
tab

TP<-tab[2,2]
FP<-tab[2,1]
FN<-tab[1,2]
TN<-tab[1,1]

accuracy=(TP+TN)/length(narr86)
specificity<-TN/(FP+TN)
sensitivity<-TP/(TP+FN)
```
h accuracy = `r accuracy`, $h_0$ specificity = `r specificity` and $h_1$ sensitivity = `r sensitivity`

############
####### IMPROVE CUTOFF BY CONSIDERING SPEC/SENS ########
###########

```{r}
pred <- prediction(fitted(log),df$arr86)
plot(performance(pred, "acc"),col="darkgreen",main="Accuracy vs. Probability cutoff")

plot(performance(pred, "sens"),col="blue",ylab="", main="Sensitivity/Specificity vs. Probability cutoff")
par(new=TRUE)
plot(performance(pred, "spec"),col="red",ylab="Specificity/Sensitivity")
legend(0.4, 0.8, legend=c("Specificity", "Sensitivity"),
       col=c("red", "blue"), lty=1:1, cex=0.8)

# -->adjusted cutoff value ... 0.3
tab_cut <- table(true= df$arr86, pred= ifelse(fitted(log)>0.3,1,0))
tab_cut
```



## ROC
```{r}
plot(performance(pred,"tpr", "fpr"))
abline(0,1,lty=2)
```

######################################
#############Jonathan###################
######################################


## Ordered Logit Model
```{r}
library("oglmx")
results.oprob<-oglmx(narr86 ~  ptime86 + qemp86 + inc86  + black + hispan + pcnvsq + pt86sq + inc86sq, data=df, link="probit",
                     constantMEAN = FALSE, constantSD = FALSE,
                     delta=0,threshparam = NULL)
summary(results.oprob)

"marginal effects"
margins.oglmx(results.oprob,ascontinuous = TRUE)

```

##WORK IN PROGRESS:Alternative model with fixed thresholds (restrictions)
```{r}

#results.oprob1<-oglmx(y ~ x1 + x2, ~ x1 + x2, data=df,
#                        constantMEAN = FALSE, constantSD = FALSE)
"Alternative model with fixed thresholds"

#results.oprob1alt<-oglmx(y ~ x1 + x2, ~ x1 + x2, data=df,
#                        constantMEAN = TRUE, constantSD = TRUE,
#                        threshparam=c(-0.5,NA,1.5))
#summary(results.oprob1)
#summary(results.oprob1alt)
```

##WORK IN PROGRESS:Some Testing
```{r}
library("lmtest")

#lrtest(results.oprob,results.oprobalt)
```
## Truncated model, let's see

######################################
#############Jonathan###################
######################################
